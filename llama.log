[1715092093] warming up the model with an empty run
[1715092095] 
llama server listening at http://127.0.0.1:8080

